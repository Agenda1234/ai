import os
import logging
from pypdf import PdfReader
from typing import List, Set
from langchain_core.documents import Document

# LangChain 1.xæ ¸å¿ƒå¯¼å…¥
from langchain_classic.chains.retrieval import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.callbacks.manager import get_openai_callback

# å…³é”®æ›¿æ¢ï¼šç”¨ChatOpenAIå¯¹æ¥é˜¿é‡Œäº‘å…¼å®¹æ¥å£ï¼ˆæ— éœ€langchain-dashscopeï¼‰
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import DashScopeEmbeddings

# æ–‡æœ¬æ‹†åˆ†ã€å‘é‡å­˜å‚¨ç›¸å…³
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ===================== é…ç½®é˜¿é‡Œäº‘APIå¯†é’¥ =====================
DASHSCOPE_API_KEY = ""
os.environ["DASHSCOPE_API_KEY"] = DASHSCOPE_API_KEY

def extract_text_with_documents(pdf) -> List[Document]:
    """è¯»å–PDFå¹¶ç”Ÿæˆå¸¦é¡µç çš„Documentå¯¹è±¡"""
    documents = []
    for page_number, page in enumerate(pdf.pages, start=1):
        page_text = page.extract_text()
        if page_text:
            doc = Document(
                page_content=page_text,
                metadata={"page_number": page_number}
            )
            documents.append(doc)
        else:
            logging.warning(f"Page {page_number} has no extractable text.")
    return documents

def process_text_with_splitter(documents: List[Document]) -> FAISS:
    """æ‹†åˆ†æ–‡æœ¬å¹¶æ„å»ºå¸¦é¡µç å…ƒæ•°æ®çš„FAISSå‘é‡åº“"""
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", " ", ""],
        chunk_size=512,
        chunk_overlap=50,
        length_function=len,
    )
    split_docs = text_splitter.split_documents(documents)
    print(f"âœ… Text splitting completed, total chunks: {len(split_docs)}")

    # åˆå§‹åŒ–é˜¿é‡Œäº‘Embedding
    embeddings = DashScopeEmbeddings(
        model="text-embedding-v3",
        dashscope_api_key=DASHSCOPE_API_KEY
    )
    knowledge_base = FAISS.from_documents(split_docs, embeddings)
    print("âœ… Vector store created using FAISS (with page metadata).")

    return knowledge_base

def test(query: str, knowledge_base: FAISS):
    """æµ‹è¯•QAé“¾ï¼ˆå…¼å®¹langchain-core 1.xï¼‰"""
    if not query:
        logging.warning("æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©ºï¼")
        return

    # å…³é”®ï¼šç”¨ChatOpenAIå¯¹æ¥é˜¿é‡Œäº‘é€šä¹‰åƒé—®å…¼å®¹æ¥å£ï¼ˆæ— éœ€langchain-dashscopeï¼‰
    llm = ChatOpenAI(
        model="qwen-turbo",  # é€šä¹‰åƒé—®æ¨¡å‹å
        api_key=DASHSCOPE_API_KEY,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        temperature=0.7,
        max_tokens=1024
    )

    # å®šä¹‰Promptæ¨¡æ¿
    prompt = ChatPromptTemplate.from_template("""
    è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œä»…ä½¿ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ å†…å®¹ï¼š
    ä¸Šä¸‹æ–‡ï¼š{context}
    é—®é¢˜ï¼š{input}
    """)

    # æ„å»ºæ–‡æ¡£åˆå¹¶é“¾
    doc_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)
    
    # æ„å»ºæ£€ç´¢+å›ç­”é“¾
    retrieval_chain = create_retrieval_chain(
        retriever=knowledge_base.as_retriever(search_kwargs={"k": 3}),
        combine_docs_chain=doc_chain
    )

    # æ‰§è¡Œé—®ç­”é“¾+æˆæœ¬ç»Ÿè®¡
    with get_openai_callback() as cost:
        response = retrieval_chain.invoke({"input": query})
        
        # è¾“å‡ºç»“æœ
        print(f"\nâœ… æŸ¥è¯¢å·²å¤„ç†ã€‚æˆæœ¬ç»Ÿè®¡ï¼š{cost}")
        print(f"ğŸ¤– å›ç­”ï¼š{response['answer']}")
        print("ğŸ“„ æ¥æºé¡µç ï¼š")
        
        # æå–å”¯ä¸€é¡µç 
        unique_pages: Set[int] = set()
        for doc in response["context"]:
            source_page = doc.metadata.get("page_number", "æœªçŸ¥")
            if source_page not in unique_pages:
                unique_pages.add(source_page)
                print(f"  - é¡µç ï¼š{source_page} | ç‰‡æ®µé¢„è§ˆï¼š{doc.page_content[:100]}...")

if __name__ == "__main__":
    try:
        # 1. è¯»å–PDF
        pdf_reader = PdfReader("agendadu.pdf")
        # 2. æå–å¸¦é¡µç çš„æ–‡æœ¬
        documents = extract_text_with_documents(pdf_reader)
        print(f"âœ… æå–åˆ°çš„æœ‰æ•ˆé¡µé¢æ•°: {len(documents)}")
        # 3. æ„å»ºå‘é‡å­˜å‚¨
        knowledge_base = process_text_with_splitter(documents)
        print("âœ… å‘é‡åº“æ„å»ºå®Œæˆï¼")

        # 4. æµ‹è¯•æ£€ç´¢
        query = "PDFä¸­ï¼Œæœè‰ºé“–çš„æ•™è‚²ç»å†æ˜¯å“ªä¸ªå¤§å­¦ï¼Ÿ"
        relevant_docs = knowledge_base.similarity_search(query, k=2)
        print(f"\nğŸ” æ£€ç´¢åˆ°{len(relevant_docs)}æ¡ç›¸å…³ç‰‡æ®µï¼š")
        for i, doc in enumerate(relevant_docs):
            print(f"  ç‰‡æ®µ{i+1}ï¼ˆé¡µç {doc.metadata['page_number']}ï¼‰ï¼š{doc.page_content[:100]}...")

        # 5. è¿è¡ŒQAé“¾æµ‹è¯•
        test(query, knowledge_base)
            
    except Exception as e:
        logging.error(f"âŒ æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
        import traceback
        traceback.print_exc()